# Explainable-AI-malware-detection

Malware detection with added explanability through saliency map on Android APK using PyTorch and Androguard.

## Requirements

To run the code you need to create a conda environment. You can do so by running the following commands after cloning the repository:

```bash
    conda create --file ./conda-package-list.yml
    conda activate malware_detection_research
```

> üóíÔ∏è **Note** : we used Androguard 3.3.5 and not the version 4.0.2 because of a bug in the submodules not being recognized by PyLance.

## How to run the code

This repository contains multiples files that can be used to train a model, test it, and generate saliency maps.
The usual workflow is:

1. Insert the APKs in the `_dataset` folder into subdirectories corresponding to their nature. For example, in our experiments we had four subdirectories: `Goodware_Obf`, `Goodware_NoObf`, `Malware_Obf`, `Malware_NoObf`. The subdirectories are used to create the dataset and the labels. The values for the name are kind of hardcoded in the code, so you might need to change them if you want to use different names. (see `model_training/train_test_resnet18.py`)
2. Run `pre_processing/apk_to_image.py` to transform the APKs into images. You have to specify the type of images you want (RGB or BW) and the padding type (random, black, white), and the extension type (jpg or png). Here's an example on how to run the script:

   ```bash
   python pre_processing/apk_to_image.py -t RGB -p random -e jpg
   ```

   This will create a folder in the `_images` corresponding to the type of conversion you chose with the nature of the APKs as subdirectories. For example, if you chose RGB and random padding, you will have the following structure:

   ```bash
   _images/Goodware_Obf_RGB_random/{apk_name}.jpg
   ```

3. Run `pre_processing/create_train_test.py` to automatically create the `train` and `test` directories with the given ratio (default 80:20 train/test).

   ```bash
   python pre_processing/create_train_test.py -r 0.8
   ```

4. Run `model_training/train_test_models.py` to train and test ResNet18 and ResNet50 model on multiple epochs. You have to specify the type of images (RGB or BW), the padding type (random, black, white) and the extension of the image (jpg or png) so the script knows where to find the images. Here's an example on how to run the script:

   ```bash
   python model_training/train_test_models.py -t RGB -p random -ex jpg
   ```

   This will save the model in the `_models` folder. The name of the model will be `{model_name}_{type}_{padding_type}_padding_{extension}.pth`. For example, if you chose RGB and random padding, you will have the following model:

   ```bash
   model_training/_models/resnet18_RGB_random_10_epochs_jpg.pth
   ...
   model_training/_models/resnet18_RGB_random_50_epochs_jpg.pth
   ...
   model_training/_models/resnet50_RGB_random_10_epochs_jpg.pth
   ...
   model_training/_models/resnet50_RGB_random_50_epochs_jpg.pth
   ```

5. Run `visualization/saliency.py` to generate the saliency maps. You have to specify the type of images (RGB or BW) and the padding type (random, black, white) with the model name (resnet18, resnet50) and finally the epochs number (10, 20, ...) Here's an example on how to run the script:

   ```bash
   python visualization/saliency.py -t RGB -p random -mn resnet18 -e 5 -ex jpg
   ```

   This will save the saliency maps in the `_saliency_maps` folder.

However, you can also use the `start_training.sh` and modify it to your needs. This script will run all the scripts in the correct order.

## Experiment tracking

If you want to see the results of our experiments with TensorBoard (the built-in VSCode way doesn't work with WSL), you can run the following command:

```bash
tensorboard --logdir=model_training/runs
```

> ‚ö†Ô∏è **Warning** : you must be at the root of the repository to run this command.

> üóíÔ∏è **Note** : you can use `tensorboard` in the CLI if you're using a precompiled TensorFlow package (e.g you installed via pip.) See [here](https://github.com/tensorflow/tensorboard/blob/master/README.md) for more details.

## Acknowledgement

This project is based on the following paper:

[1] [Obfuscation detection for Android applications](https://github.com/alevit33/apk_obfuscation_detector/tree/master#obfuscation-detection-for-android-applications) : We used `create_image.py` and `map-saturation.png` to transform the APKs into images while developing our own method.

[2] [Fast adversarial training using FGSM](https://github.com/Raideeen/fast_adversarial) Based on paper [Fast is better than free: Revisiting adversarial training](https://arxiv.org/abs/2001.03994) by Athalye et al. We used `fast_adversarial.py` to train our model.
