import os

import matplotlib.pyplot as plt
import torch
import torchvision
from colorama import Fore, Style
from torch import nn
from tqdm import tqdm


def normalize(tensor):
    tensor = tensor - tensor.min()
    tensor = tensor / tensor.max()
    return tensor


def check_saliency_values(tensor):
    print(
        f"{Fore.GREEN}[INFO]{Style.RESET_ALL} Min value: {tensor.min().item()} | Max value: {tensor.max().item()} | Mean value: {tensor.mean().item()} | Std deviation: {tensor.std().item()}\n"
    )


def create_resnet18(device: torch.device):
    # 1. Get the base model with pretrained weights and send to target device
    weights = torchvision.models.ResNet18_Weights.DEFAULT
    model = torchvision.models.resnet18(weights=weights).to(device)

    # 2. Freeze the base layers
    for param in model.parameters():
        param.requires_grad = False

    # 3. Change the classifier head
    model.fc = torch.nn.Sequential(
        nn.Linear(512, 500),
        nn.ReLU(
            inplace=True
        ),  # Changed from Mish to ReLU; you can revert this if you have a specific reason to use Mish.
        nn.Linear(500, 2),
    ).to(device)

    # 4. Give the model a name
    model.name = "resnet18"
    print(f"{Fore.GREEN}[INFO]{Style.RESET_ALL} Created new {model.name} model.")
    return model, weights


def create_resnet50(device: torch.device):
    # 1. Get the base model with pretrained weights and send to target device (GPU or CPU)
    weights = torchvision.models.ResNet50_Weights.DEFAULT
    model = torchvision.models.resnet50(weights=weights).to(device)

    # 2. Freeze the base model layers
    for param in model.parameters():
        param.requires_grad = False

    # 3. Change the classifier head
    model.fc = nn.Sequential(
        nn.Linear(2048, 500),
        nn.ReLU(inplace=True),
        nn.Linear(500, 2),
    ).to(device)

    # 4. Give the model a name
    model.name = "resnet50"
    print(f"{Fore.GREEN}[INFO]{Style.RESET_ALL} Created model: {model.name}")
    return model, weights


def compute_saliency_maps(X, y, model, criterion):
    """
    Compute a class saliency map using the model for images X and labels y.
    Input:
    - X: Input images; torch Tensor of shape (N, 3, H, W)
    - y: Labels for X; torch LongTensor of shape (N,)
    - model: A pretrained CNN that will be used to compute the saliency map.
    Returns:
    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input images.
    """
    model.eval()
    X.requires_grad_()

    scores = model(X)
    loss = criterion(scores, y)
    loss.backward()

    saliency, _ = X.grad.data.abs().max(dim=1)
    return saliency


def save_image_and_saliency(img_idx, dataloader, folder, model, device, criterion):
    X_tensor, y_tensor = dataloader.dataset[img_idx]
    X_tensor = X_tensor.unsqueeze(0).to(device)
    y_tensor = torch.tensor([y_tensor]).to(device)

    # Compute saliency
    saliency = compute_saliency_maps(X_tensor, y_tensor, model, criterion)

    # Save original image
    original_img_path = os.path.join(folder, f"Original_{img_idx}.png")
    plt.imshow(normalize(X_tensor[0]).permute(1, 2, 0).cpu().detach().numpy())
    plt.axis("off")
    plt.savefig(original_img_path)
    plt.close()

    # Save saliency map
    saliency_img_path = os.path.join(folder, f"Saliency_{img_idx}.png")
    plt.imshow(normalize(saliency[0]).cpu(), cmap=plt.cm.hot)
    plt.colorbar()
    plt.axis("off")
    plt.savefig(saliency_img_path)
    plt.close()


def save_images_and_saliencies(
    dataloader, goodware_dir, malware_dir, model, device, criterion
):
    for img_idx in tqdm(
        range(len(dataloader.dataset)), desc="Creating saliency maps..."
    ):
        label = dataloader.dataset[img_idx][1]

        match label:
            case 0:
                save_image_and_saliency(
                    img_idx,
                    dataloader,
                    goodware_dir,
                    model,
                    device,
                    criterion,
                )
            case 1:
                save_image_and_saliency(
                    img_idx,
                    dataloader,
                    malware_dir,
                    model,
                    device,
                    criterion,
                )
            case _:
                raise ValueError("Invalid label. Available labels are 0 and 1.")


def compute_and_aggregate_saliency(
    img_idx, dataloader, combined_array, model, device, criterion
):
    X_tensor, y_tensor = dataloader.dataset[img_idx]
    X_tensor = X_tensor.unsqueeze(0).to(device)
    y_tensor = torch.tensor([y_tensor]).to(device)

    # Compute saliency
    saliency = compute_saliency_maps(X_tensor, y_tensor, model, criterion)

    # Aggregate
    combined_array += saliency[0].cpu()

    return combined_array


def aggregate_saliencies(
    dataloader, goodware_dir, malware_dir, model, device, criterion
):
    # Initialize sum arrays for feature importance maps
    combined_saliency_goodware = torch.zeros_like(dataloader.dataset[0][0])
    combined_saliency_malware = torch.zeros_like(dataloader.dataset[0][0])

    goodware_count = 0
    malware_count = 0

    for img_idx in tqdm(
        range(len(dataloader.dataset)), desc="Aggregating saliency maps..."
    ):
        label = dataloader.dataset[img_idx][1]

        match label:
            case 0:
                combined_saliency_goodware = compute_and_aggregate_saliency(
                    img_idx,
                    dataloader,
                    combined_saliency_goodware,
                    model,
                    device,
                    criterion,
                )
                goodware_count += 1
            case 1:
                combined_saliency_malware = compute_and_aggregate_saliency(
                    img_idx,
                    dataloader,
                    combined_saliency_malware,
                    model,
                    device,
                    criterion,
                )
                malware_count += 1
            case _:
                raise ValueError("Invalid label. Available labels are 0 and 1.")

    # Check counts to avoid division by zero
    if goodware_count == 0 or malware_count == 0:
        raise ValueError("Dataset does not contain both labels.")

    # Average the combined saliency maps
    avg_saliency_goodware = normalize(combined_saliency_goodware / goodware_count)
    avg_saliency_malware = normalize(combined_saliency_malware / malware_count)

    # ? Check the statistics of the average saliency maps to explain the
    # ? activation colors. We can see that there is only extreme values
    # ? (close to 0 or 1) and no values in between. Resulting in a BW image. ?
    print(
        f"{Fore.YELLOW}[HEADER]{Style.RESET_ALL} Pixel distribution for goodware aggregated saliency map:"
    )
    check_saliency_values(avg_saliency_goodware)
    print(
        f"{Fore.YELLOW}[HEADER]{Style.RESET_ALL} Pixel distribution for malware aggregated saliency map:"
    )
    check_saliency_values(avg_saliency_malware)

    # Save the combined feature importance maps as images
    plt.imsave(
        os.path.join(goodware_dir, "Average_Saliency_Goodware.png"),
        avg_saliency_goodware.permute(1, 2, 0)
        .cpu()
        .detach()
        .numpy(),  # Permute dimensions
        cmap=plt.cm.hot,
    )
    plt.imsave(
        os.path.join(malware_dir, "Average_Saliency_Malware.png"),
        avg_saliency_malware.permute(1, 2, 0)
        .cpu()
        .detach()
        .numpy(),  # Permute dimensions
        cmap=plt.cm.hot,
    )
