import argparse
import os
from pathlib import Path

import torch
import torchvision
from colorama import Fore, Style
from helper import data_setup, utils
from torch import nn
from torch.utils.data import ConcatDataset, DataLoader

# Make the script interactive through the CLI
parser = argparse.ArgumentParser(
    description="Script to convert APK images into saliency maps.",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)

# Add the type of images the APK files should be converted to
parser.add_argument(
    "-t",
    "--type",
    choices=["RGB", "BW"],
    required=True,
    help="Type of images.",
)

# Add the padding type to use
parser.add_argument(
    "-p",
    "--padding_type",
    choices=["random", "black", "white"],
    required=True,
    help="Type of padding to used to create the images.",
)

parser.add_argument("-e", "--epochs", type=int, required=True, help="Number of epochs.")
parser.add_argument("-mn", "--model_name", type=str, required=True, help="Model name.")
parser.add_argument("-ex", "--extension_type", choices=["jpg", "png"], default="jpg")
parser.add_argument("-b", "--batch_size", type=int, default=32, help="Batch size.")

# Add location to the root location of the images to be saved at
parser.add_argument(
    "-i",
    "--images",
    nargs="?",
    default="_images",
    help="Provide the root path of the images to be saved at. It is a path relative to your current working directory.",
)

# Add choice on the location of the saliency maps to be saved at
parser.add_argument(
    "-s",
    "--saliency_maps",
    nargs="?",
    default="visualization/_saliency_maps",
    help="Provide the root path of the saliency maps to be saved at. It is a path relative to your current working directory.",
)

# Add location of where the model is saved
parser.add_argument(
    "-m",
    "--model_location",
    nargs="?",
    default="model_training/_models",
    help="Provide the root path of the models location. It is a path. It is a path relative to your current working directory.",
)

args = parser.parse_args()

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Setup directories
image_path = Path("_images")
train_dir = image_path / "train"
test_dir = image_path / "test"

save_directory = f"{args.saliency_maps}/saliency_maps_{args.model_name.lower()}_{args.type}_{args.padding_type}_padding_{args.extension_type}"
goodware_dir = os.path.join(
    save_directory,
    f"Goodwares_{args.model_name.lower()}_{args.type}_{args.padding_type}_padding",
)

malware_dir = os.path.join(
    save_directory,
    f"Malwares_{args.model_name.lower()}_{args.type}_{args.padding_type}_padding",
)

os.makedirs(save_directory, exist_ok=True)
os.makedirs(goodware_dir, exist_ok=True)
os.makedirs(malware_dir, exist_ok=True)

# Define the model names and their corresponding weights and model creation functions
model_info = {
    "resnet18": {
        "weights": torchvision.models.ResNet18_Weights.DEFAULT,
        "create_model": utils.create_resnet18,
    },
    "resnet50": {
        "weights": torchvision.models.ResNet50_Weights.DEFAULT,
        "create_model": utils.create_resnet50,
    },
}

# Check if the model name is valid
if args.model_name not in model_info:
    raise ValueError("Invalid model name. Available models are resnet18 and resnet50.")

# Get the model information based on the model name
model_info = model_info[args.model_name]
weights = model_info["weights"]
create_model = model_info["create_model"]

# Normalization and transformation based on the ImageNet dataset training images
automatic_transforms = weights.transforms()

# Create dataloader
train_dataset, test_dataset, class_names = data_setup.create_datasets(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=automatic_transforms,
)

all_dataloader = ConcatDataset([train_dataset, test_dataset])
all_dataloader = DataLoader(all_dataloader, batch_size=args.batch_size)

# Create the pretrained model
model, _ = create_model(device=device)

model.load_state_dict(
    torch.load(
        f"model_training/_models/{args.model_name.lower()}_{args.type}_{args.padding_type}_{args.epochs}_epochs.pth"
    )
)

# Create the loss function
criterion = nn.CrossEntropyLoss()

print(f"{Fore.GREEN}[INFO]{Style.RESET_ALL} all_dataloader: ", all_dataloader)
print(
    f"{Fore.GREEN}[INFO]{Style.RESET_ALL} Number of classes: {len(class_names)}, class names: {class_names}"
)
print(
    f"{Fore.GREEN}[INFO]{Style.RESET_ALL} automatic_transforms: ", automatic_transforms
)

# * Creating the saliency maps
utils.save_images_and_saliencies(
    dataloader=all_dataloader,
    goodware_dir=goodware_dir,
    malware_dir=malware_dir,
    model=model,
    device=device,
    criterion=criterion,
)

# * Create the average saliency maps
utils.aggregate_saliencies(
    dataloader=all_dataloader,
    goodware_dir=goodware_dir,
    malware_dir=malware_dir,
    model=model,
    device=device,
    criterion=criterion,
)
