import argparse
import os
from pathlib import Path

import torch
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from torchvision import transforms
from tqdm import tqdm

parser = argparse.ArgumentParser(
    description="Script to train and test DexRay model",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)

parser.add_argument(
    "-lr",
    "--learning_rate",
    choices=[0.001, 0.0001],
    type=float,
    default=0.0001,
    help="learning rate",
)
parser.add_argument(
    "-p",
    "--proba",
    choices=[0, 0.1, 0.2, 0.3, 0.4, 0.5],
    type=float,
    default=0.2,
    help="dropout_rate",
)

args = parser.parse_args()


# Initialize TensorBoard writer
experiment_name = "DexRay"
model_name = f"lr_{args.learning_rate}_p_{args.proba}"
writer = engine.create_writer(experiment_name=experiment_name, model_name=model_name)

# Create the directories if they don't exist
os.makedirs("model_training/_models", exist_ok=True)

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

IMG_SIZE = 128
BATCH_SIZE = 240
EPOCHS = 200

# Set up directories
image_path = Path("../_resized_vector_128")
train_dir = image_path / "train"
test_dir = image_path / "test"


transform = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Lambda(lambda x: x.view(-1)),  # Flatten the image
    ]
)


train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=transform,
    batch_size=BATCH_SIZE,
)


# Initialize the model
model = models.DexRayModel(IMG_SIZE, args.proba).to(device)
loss_fn = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


# Training Loop
for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in tqdm(
        enumerate(train_dataloader),
        total=len(train_dataloader),
        desc=f"Epoch {epoch+1}/{EPOCHS}",
    ):
        inputs, labels = inputs.to(device), labels.to(device).float()

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        writer.add_scalar(
            "Training Loss", loss.item(), epoch * len(train_dataloader) + i
        )

    print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_dataloader):.4f}")

    # Save the model checkpoint
    # checkpoint_path = f"_models/{model_name}_epoch_{epoch+1}.pth"
    # torch.save(model.state_dict(), checkpoint_path)


# Save the trained model
save_filepath = f"_models/{model_name}.pth"
utils.save_model(model=model, target_dir="model_training", model_name=save_filepath)


# Evaluate the model
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for i, (inputs, labels) in tqdm(
        enumerate(test_dataloader), total=len(test_dataloader), desc="Evaluating"
    ):
        inputs, labels = inputs.to(device), labels.to(device).float()

        outputs = model(inputs)
        preds = torch.sigmoid(outputs).squeeze() > 0.5
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_preds)
recall = recall_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds)


print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"F1 Score: {f1:.2f}")


# Append model's performance to "performance.txt"
with open("performance.txt", "a") as f:
    f.write(f"Model: {model_name}\n")
    f.write(f"Accuracy: {accuracy:.4f}\n")
    f.write(f"F1 Score: {f1:.2f}\n")
    f.write(f"Recall: {recall:.2f}\n")
    f.write(f"Precision: {precision:.2f}\n")
    f.write("\n")  # Add a newline for readability


writer.close()
