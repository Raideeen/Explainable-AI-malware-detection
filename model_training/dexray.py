import os
import torch
from torchvision import transforms
from pathlib import Path
from helper import data_setup, engine, models, utils
from tqdm import tqdm
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score


# Note : for the moment, we only focus on BW .png images

# Initialize TensorBoard writer
experiment_name = "DexRay_training"
model_name = "DexRayModel"
writer = engine.create_writer(experiment_name=experiment_name, model_name=model_name)

# Create the directories if they don't exist
os.makedirs("model_training/_models", exist_ok=True)

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

IMG_SIZE = 128
BATCH_SIZE = 32
EPOCHS = 20

# Set up directories
image_path = Path("../_vector_images")
train_dir = image_path / "train"
test_dir = image_path / "test"


print("Constants set")

transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize the image
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(1, -1))  # Flatten the image
])


train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(
    train_dir=train_dir,
    test_dir=test_dir,
    transform=transform,
    batch_size=BATCH_SIZE,
)

print("Dataloader created")

# Initialize the model
model = models.DexRayModel(IMG_SIZE).to(device)
loss_fn = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

print("Starting the training...")

# Training Loop
for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0
    for i, (inputs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f"Epoch {epoch+1}/{EPOCHS}"):
        inputs, labels = inputs.to(device), labels.to(device).float()

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward + backward + optimize
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        
        writer.add_scalar('Training Loss', loss.item(), epoch * len(train_dataloader) + i)

    print(f"Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_dataloader):.4f}")

    # Save the model checkpoint
    #checkpoint_path = f"_models/{model_name}_epoch_{epoch+1}.pth"
    #torch.save(model.state_dict(), checkpoint_path)


# Save the trained model
save_filepath = f"_models/{model_name}.pth"
utils.save_model(
    model=model, target_dir="model_training", model_name=save_filepath
)


# Evaluate the model
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for i, (inputs, labels) in tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc="Evaluating"):
        inputs, labels = inputs.to(device), labels.to(device).float()

        outputs = model(inputs)
        preds = torch.sigmoid(outputs).squeeze() > 0.5
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_preds)
recall = recall_score(all_labels, all_preds)
precision = precision_score(all_labels, all_preds)
f1 = f1_score(all_labels, all_preds)


print(f"Accuracy: {accuracy:.4f}")
print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"F1 Score: {f1:.2f}")
        

# Append model's performance to "performance.txt"
with open("performance.txt", "a") as f:
    f.write(f"Model: {model_name}\n")
    f.write(f"Accuracy: {accuracy:.4f}")
    f.write(f"F1 Score: {f1:.2f}\n")
    f.write(f"Recall: {recall:.2f}\n")
    f.write(f"Precision: {precision:.2f}\n")
    f.write("\n")  # Add a newline for readability
    
    
writer.close()

